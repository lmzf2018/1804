
#ELK

elasticsearch 单机安装
1 修改 /etc/hosts
192.168.1.11	es1
192.168.1.12	es2
192.168.1.13	es3
192.168.1.14	es4
192.168.1.15	es5
192.168.1.18	kibana
192.168.1.20	logstash
192.168.1.33	web01

2 安装 yum install -y java-1.8.0-openjdk
3 安装 yum install -y elasticsearch
修改配置文件 /etc/elasticsearch/elasticsearch.yml
network.host: 0.0.0.0
启动服务 systemctl start elasticsearch

浏览器访问验证 http://192.168.1.11:9200/

在所有节点安装 
yum install -y java-1.8.0-openjdk elasticsearch

修改配置文件
cluster.name: nsd1804
node.name: 本机主机名称
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]

验证集群
http://192.168.1.11:9200/_cluster/health?pretty

插件安装
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/bigdesk-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-head-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-kopf-master.zip

访问插件 head
http://192.168.1.15:9200/_plugin/head
访问插件 kopf
http://192.168.1.15:9200/_plugin/kopf
访问插件 bigdesk
http://192.168.1.15:9200/_plugin/bigdesk

创建索引
curl -XPUT 'http://192.168.1.13:9200/tedu/' -d \
'{
    "settings":{
        "index":{
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    }
}'

增加数据 PUT
curl -XPUT "http://192.168.1.11:9200/nsd1804/teacher/2" -d \
'{
  "title": "阶段2",
  "name": {"first":"老逗比", "last":"丁丁"},
  "age":52
}'

更改数据 POST
curl -XPOST "http://192.168.1.15:9200/nsd1804/teacher/3/_update" -d \
'{
  "doc": { "age":18 }
}'

查询与删除数据
curl -XGET    "http://192.168.1.14:9200/nsd1804/teacher/1?pretty"
curl -XDELETE "http://192.168.1.14:9200/nsd1804/teacher/1?pretty"

kibana 配置 /opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://es1:9200"
kibana.index: ".kibana"
kibana.defaultAppId: "discover"
elasticsearch.pingTimeout: 1500
elasticsearch.requestTimeout: 30000
elasticsearch.startupTimeout: 5000

数据批量导入
有 index, type, id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @shakespeare.json

无 index, type 有 id 的导入
curl -XPOST http://192.168.1.13:9200/oo/xx/_bulk --data-binary @accounts.json

有 多个index,type 无 id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @logs.jsonl

数据批量查询
curl -XGET http://192.168.1.12:9200/_mget?pretty -d '
{ 
  docs:[
    {"_index": "oo",
     "_type": "xx",
     "_id": 99
    },
    {"_index": "shakespeare",
     "_type": "act",
     "_id": 80730
    },
    {"_index": "logstash-2015.05.20",
     "_type": "log",
     "_id": "AWTo2xXm16RGslV6jxJR"
    }
  ]
}'

插件文档地址
https://www.elastic.co/guide/en/logstash/current/index.html
源码地址 https://github.com/logstash-plugins

logstash 配置文件 logstash.conf 
input{
  stdin{ codec => "json" }
}

filter{  }

output{
  stdout{ codec => "rubydebug" }
}

file 模块
input {
  ... ...
  file {
    path => ["/tmp/a.log", "/tmp/b.log"]
    sincedb_path => "/var/lib/logstash/sincedb.log"
    start_position => "beginning"
    type => "filelog"
  }
}

tcp & udp 模块
input {
  ... ...
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
}

测试命令
echo "test udp log" >/dev/udp/192.168.1.20/8888
echo "test tcp log" >/dev/tcp/192.168.1.20/8888

syslog 模块

input {
  ... ...
  syslog {
    port => 514
    type => "syslog"
  }
}

客户机配置 @@(tcp)  @(udp)
local0.info                     @@192.168.1.20:514
命令
logger -p local0.info -t "testlog" "hello world"

正则表达式分组匹配 (?<name>reg)

正则宏路径
/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns

filter{
  grok{
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }
}

使用 filebeat 收集日志，发送到 logstash
logstash beats 配置
input{
  ... ...
  beats{
    port => 5044
  }
}

客户端配置
filebeat:
  prospectors:
    -
      paths:
        - /var/log/httpd/access_log
      input_type: log
      document_type: apachelog
  registry_file: /var/lib/filebeat/registry
output:
  logstash:
    hosts: ["192.168.1.20:5044"]
shipper:
logging:
  files:
    rotateeverybytes: 10485760 # = 10MB

elastic 配置，及完整配置文件
input {
  stdin{ codec => "json" }
  file {
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb"
    path => ["/tmp/a.log"]
    type => "test_file_log"
  } 
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
  syslog {
    port => 514
    type => "syslog"
  }
  beats{
    port => 5044
  }
}

filter{
  if [type] == "apachelog"{
  grok {
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }}
}

output{
  stdout{ codec => "rubydebug" }
  if [type] == "apachelog"{
  elasticsearch {
    hosts => ["192.168.1.11:9200", "192.168.1.12:9200"]
    index => "weblog"
    flush_size => 2000
    idle_flush_time => 10
  }}
}

-----------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------

rsync

192.168.1.144  node1
192.168.1.101  node2
192.168.1.183  node3
192.168.1.193  node4
192.168.1.171  node5
192.168.1.116  node6


vim  /etc/sysconfig/network-scripts/ifcfg-eth0

TYPE="Ethernet"
BOOTPROTO="static"
IPADDR=192.168.1.144
PREFIX=24
GATEWAY=192.168.1.254
DEVICE="eth0"
ONBOOT="yes"

systemctl  restart  network

#man帮助
man  5  hosts_access







vim  /etc/ansible/host

## db-[99:101]-node.example.com
[web]
web1
web2

[db]
db[1:2]

[app:children]
web
db


[app:vars]
ansible_ssh_user="root"
ansible_ssh_pass="1"

[other]
cache  ansible_ssh_user="root" ansible_ssh_pass="1"

______________________________________________________________________________

vim  /etc/ansible/host

## db-[99:101]-node.example.com
[web]
web1
web2

[db]
db[1:2]

[app:children]
web
db

[other]
cache 


[root@ansible ~]# ansible  all  -m  command  -a  'uptime' -k
SSH password: 
db2 | SUCCESS | rc=0 >>
 16:02:21 up  4:34,  1 user,  load average: 0.00, 0.01, 0.05

web2 | SUCCESS | rc=0 >>
 16:02:21 up  4:34,  1 user,  load average: 0.00, 0.01, 0.05

web1 | SUCCESS | rc=0 >>
 16:02:21 up  4:34,  1 user,  load average: 0.00, 0.01, 0.05

db1 | SUCCESS | rc=0 >>
 16:02:21 up  4:34,  1 user,  load average: 0.16, 0.05, 0.06

cache | SUCCESS | rc=0 >>
 16:02:21 up  4:34,  1 user,  load average: 0.00, 0.01, 0.05





[root@ansible ~]# cd  /root/.ssh/
[root@ansible .ssh]# ls
authorized_keys  known_hosts
[root@ansible .ssh]# cat  authorized_keys 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCv3uPUOnpaaE9/l5jkUEp5K3ZsP3dgrRfkJur5UkmPvgGoz4UGV4vd2Dna1R6En2mmTDEwFyqmpJs59Hgfw8rTF8EbDRepJ7N4+mm3sUYnFy9BJgafJkK84ZdQ1yA/A64LzVg02PwHqmSO5eLnAtWmeToIlDvyowaZou6YJqSdYflJDTu6793PshxhwdTFyK1NqIS4mfHHpWgm/oObXRCmqbvB4EiACuasvXN46JKTpa0COezdjUKDmyVspSMFdN7jpEuWZPMPSnOyv/FDSYjEbqoZpiIQMSRBVDbNsz7E+mViP1yDi5nJ/9nmbIZLgbykEU0gEOAxKSZXvjbDXn2p root@weekend9pc01.tedu.cn

[root@ansible .ssh]# ansible all  -m  authorized_key -a "user=root exclusive=true manage_dir=true  key='$(</root/.ssh/id_rsa.pub)'"   -k
SSH password: 
web2 | SUCCESS => {
    "changed": true, 
    "comment": null, 
    "exclusive": true, 
    "key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCxaFfwxVAc6WwSVUlzKOZDWzx9QrWudPw5iOrl/TySW2PX/et9i0bQJMw6jS4G96i7lnaSzErZdEhANp8CiRr0RVdIThUmZplN9yzdoXSmDYl/mPg9crQX/yV55nOa3Xv5TpbEEBBU7OMgI2jX83ubLCnotZVkCqE2QlaP056O9UpjoYwqJMW9qOFONv0rPhrY9Vb7Cz3icoFunbQi8yPbbnqHvw330zJpVqekieAALs01cpgiI0EwYkK2KNJ+fTNBWukyv0vWM6i9jScGMGH2i5M/pNcxqQT/QBrwrwIZUJstu1G/CqfUniwhjzoYZ9o6m9s3dYiehG47wpwKOdfB root@ansible", 
    "key_options": null, 
    "keyfile": "/root/.ssh/authorized_keys", 
    "manage_dir": true, 
    "path": null, 
    "state": "present", 
    "unique": false, 
    "user": "root", 
    "validate_certs": true
}
db2 | SUCCESS => {
    "changed": true, 
    "comment": null, 
    "exclusive": true, 
    "key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCxaFfwxVAc6WwSVUlzKOZDWzx9QrWudPw5iOrl/TySW2PX/et9i0bQJMw6jS4G96i7lnaSzErZdEhANp8CiRr0RVdIThUmZplN9yzdoXSmDYl/mPg9crQX/yV55nOa3Xv5TpbEEBBU7OMgI2jX83ubLCnotZVkCqE2QlaP056O9UpjoYwqJMW9qOFONv0rPhrY9Vb7Cz3icoFunbQi8yPbbnqHvw330zJpVqekieAALs01cpgiI0EwYkK2KNJ+fTNBWukyv0vWM6i9jScGMGH2i5M/pNcxqQT/QBrwrwIZUJstu1G/CqfUniwhjzoYZ9o6m9s3dYiehG47wpwKOdfB root@ansible", 
    "key_options": null, 
    "keyfile": "/root/.ssh/authorized_keys", 
    "manage_dir": true, 
    "path": null, 
    "state": "present", 
    "unique": false, 
    "user": "root", 
    "validate_certs": true
}
web1 | SUCCESS => {
    "changed": true, 
    "comment": null, 
    "exclusive": true, 
    "key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCxaFfwxVAc6WwSVUlzKOZDWzx9QrWudPw5iOrl/TySW2PX/et9i0bQJMw6jS4G96i7lnaSzErZdEhANp8CiRr0RVdIThUmZplN9yzdoXSmDYl/mPg9crQX/yV55nOa3Xv5TpbEEBBU7OMgI2jX83ubLCnotZVkCqE2QlaP056O9UpjoYwqJMW9qOFONv0rPhrY9Vb7Cz3icoFunbQi8yPbbnqHvw330zJpVqekieAALs01cpgiI0EwYkK2KNJ+fTNBWukyv0vWM6i9jScGMGH2i5M/pNcxqQT/QBrwrwIZUJstu1G/CqfUniwhjzoYZ9o6m9s3dYiehG47wpwKOdfB root@ansible", 
    "key_options": null, 
    "keyfile": "/root/.ssh/authorized_keys", 
    "manage_dir": true, 
    "path": null, 
    "state": "present", 
    "unique": false, 
    "user": "root", 
    "validate_certs": true
}
db1 | SUCCESS => {
    "changed": true, 
    "comment": null, 
    "exclusive": true, 
    "key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCxaFfwxVAc6WwSVUlzKOZDWzx9QrWudPw5iOrl/TySW2PX/et9i0bQJMw6jS4G96i7lnaSzErZdEhANp8CiRr0RVdIThUmZplN9yzdoXSmDYl/mPg9crQX/yV55nOa3Xv5TpbEEBBU7OMgI2jX83ubLCnotZVkCqE2QlaP056O9UpjoYwqJMW9qOFONv0rPhrY9Vb7Cz3icoFunbQi8yPbbnqHvw330zJpVqekieAALs01cpgiI0EwYkK2KNJ+fTNBWukyv0vWM6i9jScGMGH2i5M/pNcxqQT/QBrwrwIZUJstu1G/CqfUniwhjzoYZ9o6m9s3dYiehG47wpwKOdfB root@ansible", 
    "key_options": null, 
    "keyfile": "/root/.ssh/authorized_keys", 
    "manage_dir": true, 
    "path": null, 
    "state": "present", 
    "unique": false, 
    "user": "root", 
    "validate_certs": true
}
cache | SUCCESS => {
    "changed": true, 
    "comment": null, 
    "exclusive": true, 
    "key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCxaFfwxVAc6WwSVUlzKOZDWzx9QrWudPw5iOrl/TySW2PX/et9i0bQJMw6jS4G96i7lnaSzErZdEhANp8CiRr0RVdIThUmZplN9yzdoXSmDYl/mPg9crQX/yV55nOa3Xv5TpbEEBBU7OMgI2jX83ubLCnotZVkCqE2QlaP056O9UpjoYwqJMW9qOFONv0rPhrY9Vb7Cz3icoFunbQi8yPbbnqHvw330zJpVqekieAALs01cpgiI0EwYkK2KNJ+fTNBWukyv0vWM6i9jScGMGH2i5M/pNcxqQT/QBrwrwIZUJstu1G/CqfUniwhjzoYZ9o6m9s3dYiehG47wpwKOdfB root@ansible", 
    "key_options": null, 
    "keyfile": "/root/.ssh/authorized_keys", 
    "manage_dir": true, 
    "path": null, 
    "state": "present", 
    "unique": false, 
    "user": "root", 
    "validate_certs": true


___________________________________________________________________________________________________


#批量开启虚拟机

 for  i  in  `virsh list  --all |  grep  -E  node? | awk '{print $2}'`; do  virsh start $i ; done

或者

 for  i  in  `virsh list  --all |  grep  -oE  node[1-9] ; do  virsh start $i ; done

___________________________________________________________________________________________________

[root@ansible ~]# ansible  all  -m  setup -a  'filter="ansible_os_family"'
cache | SUCCESS => {
    "ansible_facts": {
        "ansible_os_family": "RedHat"
    }, 
    "changed": false
}
web2 | SUCCESS => {
    "ansible_facts": {
        "ansible_os_family": "RedHat"
    }, 
    "changed": false
}
db2 | SUCCESS => {
    "ansible_facts": {
        "ansible_os_family": "RedHat"
    }, 
    "changed": false
}
db1 | SUCCESS => {
    "ansible_facts": {
        "ansible_os_family": "RedHat"
    }, 
    "changed": false
}
web1 | SUCCESS => {
    "ansible_facts": {
        "ansible_os_family": "RedHat"
    }, 
    "changed": false

_____________________________________________________________________________


playbook练习
• 编写 playbook 实现以下效果
– 安装 apache
– 修改 apache 监听的端口为 8080
– 为 apache 增加 NameServer 配置
– 设置默认主页 hello world
– 吭劢服务
– 设置开机自吭劢
 

[root@ansible ~]#  vim  httpd.yml

---
- hosts: web
  remote_user: root
  tasks:
    - name: http_create
      yum: name=httpd state=installed
    - lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^Listen'
        insertafter: '^#Listen '
        line: 'Listen 8080'
        mode: 0644
    - lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^SeverName'
        line: 'ServerName  localhost'
    - shell: echo 'hello'  > /var/www/html/index.html
    - service:
        name: httpd
        state: restarted
____________________________________________________________________________

#ELK

# vim  /etc/hosts

192.168.1.11  es1
192.168.1.12  es2
192.168.1.13  es3
192.168.1.14  es4
192.168.1.15  es5
192.168.1.16  kibana
192.168.1.17  logstash
192.168.1.18  web01


[root@logstash logstash]# vim  logstash.conf

input{
 stdin{ codec => "json" }

file{
start_position => "beginning"
sincedb_path => "/var/lib/logstash/sincedb-access"
path => [ "/tmp/alog", "/tmp/blog" ]
type => 'filelog'}

  }

 
filter{
   }

output{
  stdout{codec => "rubydebug" }
  }

[root@logstash ~]# echo  hao  >> /tmp/alog


[root@logstash logstash]# logstash  -f  logstash.conf
Settings: Default pipeline workers: 2
Pipeline main started
{
       "message" => "hao",
      "@version" => "1",
    "@timestamp" => "2018-08-29T03:36:14.791Z",
          "path" => "/tmp/alog",
          "host" => "logstash",
          "type" => "filelog"
}


___________________________________________________________________________________




[root@logstash logstash]# /etc/logstash/logstash.conf

input{
 stdin{ codec => "json" }

 file{
 start_position => "beginning"
 sincedb_path => "/var/lib/logstash/sincedb-access"
 path => [ "/tmp/alog", "/tmp/blog" ]
 type => 'filelog'}


 tcp {
 mode => "server"
 host => "0.0.0.0"
 port => 8888
 type => "tcplog"
 }
 udp {
 port => 8888elasticsearch 单机安装
1 修改 /etc/hosts
192.168.1.11	es1
192.168.1.12	es2
192.168.1.13	es3
192.168.1.14	es4
192.168.1.15	es5
192.168.1.18	kibana
192.168.1.20	logstash
192.168.1.33	web01

2 安装 yum install -y java-1.8.0-openjdk
3 安装 yum install -y elasticsearch
修改配置文件 /etc/elasticsearch/elasticsearch.yml
network.host: 0.0.0.0
启动服务 systemctl start elasticsearch

浏览器访问验证 http://192.168.1.11:9200/

在所有节点安装 
yum install -y java-1.8.0-openjdk elasticsearch

修改配置文件
cluster.name: nsd1804
node.name: 本机主机名称
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]

验证集群
http://192.168.1.11:9200/_cluster/health?pretty
elasticsearch 单机安装
1 修改 /etc/hosts
192.168.1.11	es1
192.168.1.12	es2
192.168.1.13	es3
192.168.1.14	es4
192.168.1.15	es5
192.168.1.18	kibana
192.168.1.20	logstash
192.168.1.33	web01

2 安装 yum install -y java-1.8.0-openjdk
3 安装 yum install -y elasticsearch
修改配置文件 /etc/elasticsearch/elasticsearch.yml
network.host: 0.0.0.0
启动服务 systemctl start elasticsearch

浏览器访问验证 http://192.168.1.11:9200/

在所有节点安装 
yum install -y java-1.8.0-openjdk elasticsearch

修改配置文件
cluster.name: nsd1804
node.name: 本机主机名称
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]

验证集群
http://192.168.1.11:9200/_cluster/health?pretty

插件安装
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/bigdesk-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-head-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-kopf-master.zip

访问插件 head
http://192.168.1.15:9200/_plugin/head
访问插件 kopf
http://192.168.1.15:9200/_plugin/kopf
访问插件 bigdesk
http://192.168.1.15:9200/_plugin/bigdesk

创建索引
curl -XPUT 'http://192.168.1.13:9200/tedu/' -d \
'{
    "settings":{
        "index":{
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    }
}'

增加数据 PUT
curl -XPUT "http://192.168.1.11:9200/nsd1804/teacher/2" -d \
'{
  "title": "阶段2",
  "name": {"first":"老逗比", "last":"丁丁"},
  "age":52
}'

更改数据 POST
curl -XPOST "http://192.168.1.15:9200/nsd1804/teacher/3/_update" -d \
'{
  "doc": { "age":18 }
}'

查询与删除数据
curl -XGET    "http://192.168.1.14:9200/nsd1804/teacher/1?pretty"
curl -XDELETE "http://192.168.1.14:9200/nsd1804/teacher/1?pretty"

kibana 配置 /opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://es1:9200"
kibana.index: ".kibana"
kibana.defaultAppId: "discover"
elasticsearch.pingTimeout: 1500
elasticsearch.requestTimeout: 30000
elasticsearch.startupTimeout: 5000

数据批量导入
有 index, type, id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @shakespeare.json

无 index, type 有 id 的导入
curl -XPOST http://192.168.1.13:9200/oo/xx/_bulk --data-binary @accounts.json

有 多个index,type 无 id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @logs.jsonl

数据批量查询
curl -XGET http://192.168.1.12:9200/_mget?pretty -d '
{ 
  docs:[
    {"_index": "oo",
     "_type": "xx",
     "_id": 99
    },
    {"_index": "shakespeare",
     "_type": "act",
     "_id": 80730
    },
    {"_index": "logstash-2015.05.20",
     "_type": "log",
     "_id": "AWTo2xXm16RGslV6jxJR"
    }
  ]
}'

插件文档地址
https://www.elastic.co/guide/en/logstash/current/index.html
源码地址 https://github.com/logstash-plugins

logstash 配置文件 logstash.conf 
input{
  stdin{ codec => "json" }
}

filter{  }

output{
  stdout{ codec => "rubydebug" }
}

file 模块
input {
  ... ...
  file {
    path => ["/tmp/a.log", "/tmp/b.log"]
    sincedb_path => "/var/lib/logstash/sincedb.log"
    start_position => "beginning"
    type => "filelog"
  }
}

tcp & udp 模块
input {
  ... ...
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
}

测试命令
echo "test udp log" >/dev/udp/192.168.1.20/8888
echo "test tcp log" >/dev/tcp/192.168.1.20/8888

syslog 模块

input {
  ... ...
  syslog {
    port => 514
    type => "syslog"
  }
}

客户机配置 @@(tcp)  @(udp)
local0.info                     @@192.168.1.20:514
命令
logger -p local0.info -t "testlog" "hello world"

正则表达式分组匹配 (?<name>reg)

正则宏路径
/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns

filter{
  grok{
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }
}

使用 filebeat 收集日志，发送到 logstash
logstash beats 配置
input{
  ... ...
  beats{
    port => 5044
  }
}

客户端配置
filebeat:
  prospectors:
    -
      paths:
        - /var/log/httpd/access_log
      input_type: log
      document_type: apachelog
  registry_file: /var/lib/filebeat/registry
output:
  logstash:
    hosts: ["192.168.1.20:5044"]
shipper:
logging:
  files:
    rotateeverybytes: 10485760 # = 10MB

elastic 配置，及完整配置文件
input {
  stdin{ codec => "json" }
  file {
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb"
    path => ["/tmp/a.log"]
    type => "test_file_log"
  } 
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
  syslog {
    port => 514
    type => "syslog"
  }
  beats{
    port => 5044
  }
}

filter{
  if [type] == "apachelog"{
  grok {
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }}
}

output{
  stdout{ codec => "rubydebug" }
  if [type] == "apachelog"{
  elasticsearch {
    hosts => ["192.168.1.11:9200", "192.168.1.12:9200"]
    index => "weblog"
    flush_size => 2000
    idle_flush_time => 10
  }}
}


插件安装
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/bigdesk-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-head-master.zip
/usr/share/elasticsearch/bin/plugin install ftp://192.168.1.254/public/elasticsearch-kopf-master.zip

访问插件 head
http://192.168.1.15:9200/_plugin/head
访问插件 kopf
http://192.168.1.15:9200/_plugin/kopf
访问插件 bigdesk
http://192.168.1.15:9200/_plugin/bigdesk

创建索引
curl -XPUT 'http://192.168.1.13:9200/tedu/' -d \
'{
    "settings":{
        "index":{
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    }
}'

增加数据 PUT
curl -XPUT "http://192.168.1.11:9200/nsd1804/teacher/2" -d \
'{
  "title": "阶段2",
  "name": {"first":"老逗比", "last":"丁丁"},
  "age":52
}'

更改数据 POST
curl -XPOST "http://192.168.1.15:9200/nsd1804/teacher/3/_update" -d \
'{
  "doc": { "age":18 }
}'

查询与删除数据
curl -XGET    "http://192.168.1.14:9200/nsd1804/teacher/1?pretty"
curl -XDELETE "http://192.168.1.14:9200/nsd1804/teacher/1?pretty"

kibana 配置 /opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://es1:9200"
kibana.index: ".kibana"
kibana.defaultAppId: "discover"
elasticsearch.pingTimeout: 1500
elasticsearch.requestTimeout: 30000
elasticsearch.startupTimeout: 5000

数据批量导入
有 index, type, id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @shakespeare.json

无 index, type 有 id 的导入
curl -XPOST http://192.168.1.13:9200/oo/xx/_bulk --data-binary @accounts.json

有 多个index,type 无 id 的导入
curl -XPOST http://192.168.1.13:9200/_bulk --data-binary @logs.jsonl

数据批量查询
curl -XGET http://192.168.1.12:9200/_mget?pretty -d '
{ 
  docs:[
    {"_index": "oo",
     "_type": "xx",
     "_id": 99
    },
    {"_index": "shakespeare",
     "_type": "act",
     "_id": 80730
    },
    {"_index": "logstash-2015.05.20",
     "_type": "log",
     "_id": "AWTo2xXm16RGslV6jxJR"
    }
  ]
}'

插件文档地址
https://www.elastic.co/guide/en/logstash/current/index.html
源码地址 https://github.com/logstash-plugins

logstash 配置文件 logstash.conf 
input{
  stdin{ codec => "json" }
}

filter{  }

output{
  stdout{ codec => "rubydebug" }
}

file 模块
input {
  ... ...
  file {
    path => ["/tmp/a.log", "/tmp/b.log"]
    sincedb_path => "/var/lib/logstash/sincedb.log"
    start_position => "beginning"
    type => "filelog"
  }
}

tcp & udp 模块
input {
  ... ...
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
}

测试命令
echo "test udp log" >/dev/udp/192.168.1.20/8888
echo "test tcp log" >/dev/tcp/192.168.1.20/8888

syslog 模块

input {
  ... ...
  syslog {
    port => 514
    type => "syslog"
  }
}

客户机配置 @@(tcp)  @(udp)
local0.info                     @@192.168.1.20:514
命令
logger -p local0.info -t "testlog" "hello world"

正则表达式分组匹配 (?<name>reg)

正则宏路径
/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns

filter{
  grok{
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }
}

使用 filebeat 收集日志，发送到 logstash
logstash beats 配置
input{
  ... ...
  beats{
    port => 5044
  }
}

客户端配置
filebeat:
  prospectors:
    -
      paths:
        - /var/log/httpd/access_log
      input_type: log
      document_type: apachelog
  registry_file: /var/lib/filebeat/registry
output:
  logstash:
    hosts: ["192.168.1.20:5044"]
shipper:
logging:
  files:
    rotateeverybytes: 10485760 # = 10MB

elastic 配置，及完整配置文件
input {
  stdin{ codec => "json" }
  file {
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb"
    path => ["/tmp/a.log"]
    type => "test_file_log"
  } 
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
  syslog {
    port => 514
    type => "syslog"
  }
  beats{
    port => 5044
  }
}

filter{
  if [type] == "apachelog"{
  grok {
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }}
}

output{
  stdout{ codec => "rubydebug" }
  if [type] == "apachelog"{
  elasticsearch {
    hosts => ["192.168.1.11:9200", "192.168.1.12:9200"]
    index => "weblog"
    flush_size => 2000
    idle_flush_time => 10
  }}
}


 type => "udplog"
 }
}

filter{
 }

output{
  stdout{codec => "rubydebug" }
  }
                                                               

[root@kibana ~]# echo  ok  > /dev/udp/192.168.1.17/8888
[root@kibana ~]# echo adads  > /dev/udp/192.168.1.17/8888
[root@kibana ~]# echo "test tcp log" >/dev/tcp/192.168.1.17/8888

[root@logstash logstash]# logstash -f  logstash.conf
Settings: Default pipeline workers: 2
Pipeline main started
{
       "message" => "ok\n",
      "@version" => "1",
    "@timestamp" => "2018-08-29T06:31:00.351Z",
          "type" => "udplog",
          "host" => "192.168.1.16"
}
{
       "message" => "adads\n",
      "@version" => "1",
    "@timestamp" => "2018-08-29T06:31:06.310Z",
          "type" => "udplog",
          "host" => "192.168.1.16"
}
{
       "message" => "test tcp log",
      "@version" => "1",
    "@timestamp" => "2018-08-29T06:31:12.157Z",
          "host" => "192.168.1.16",
          "port" => 42518,
          "type" => "tcplog"
}


______________________________________________________________________


vim  /etc/rsyslog.conf


[root@logstash logstash]# logger --help


_______________________________________________________________________

[root@kibana ~]# vim  /etc/rsyslog.conf 


...
# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log

local0.info                     @@192.168.1.17:514    #自己添加

[root@kibana ~]# systemctl   restart  rsyslog
[root@kibana ~]# logger -p local0.info -t "testlog" "hello world"
[root@logstash logstash]# vim logstash.conf
input{
 stdin{ codec => "json" }192.168.1.11  es1
192.168.1.12  es2
192.168.1.13  es3
192.168.1.14  es4
192.168.1.15  es5
192.168.1.16  kibana
192.168.1.17  logstash
192.168.1.18  web01

 file{
 start_position => "beginning"
 sincedb_path => "/var/lib/logstash/sincedb-access"
 path => [ "/tmp/alog", "/tmp/blog" ]
 type => 'filelog'}


 tcp {
 mode => "server"
 host => "0.0.0.0"
 port => 8888
 type => "tcplog"
 }
 udp {
 port => 8888
 type => "udplog"
 }

 syslog {
    port => 514
    type => "syslog"
  }

}
filter{
 }

  output{
  stdout{codec => "rubydebug" }
  }

[root@logstash logstash]# logstash -f  logstash.conf
Settings: Default pipeline workers: 2
Pipeline main starte
{
           "message" => "hello world\n",
          "@version" => "1",
        "@timestamp" => "2018-08-29T06:52:31.000Z",
              "type" => "syslog",
              "host" => "192.168.1.16",
          "priority" => 134,
         "timestamp" => "Aug 29 14:52:31",
         "logsource" => "kibana",
           "program" => "testlog",
          "severity" => 6,
          "facility" => 16,
    "facility_label" => "local0",
    "severity_label" => "Informational"
}



____________________________________________________________________________________

#logstash 部分

[root@logstash logstash]# vim  /tmp/alog
192.168.1.18 - - [29/Aug/2018:15:00:38 +0800] "GET / HTTP/1.1" 200 3 "-" "curl/7.29.0"


[root@logstash logstash]# vim logstash.conf

input{
 stdin{ codec => "json" }

 file{
 start_position => "beginning"
 #sincedb_path => "/var/lib/logstash/sincedb-access"       #注释掉
 sincedb_path => "/dev/null"                               #添加
 path => [ "/tmp/alog"]
 type => 'filelog'}


 tcp {
 mode => "server"
 host => "0.0.0.0"
 port => 8888
 type => "tcplog"
 }
 udp {
 port => 8888
 type => "udplog"
 }
 syslog {
    port => 514
    type => "syslog"
  }

}
filter{
  grok{                                           #
    match => ["message", "%{COMBINEDAPACHELOG}"]  #
  }                                               #

 }

  output{
  stdout{codec => "rubydebug" }
  }


[root@logstash logstash]# logstash -f  logstash.conf
Settings: Default pipeline workers: 2
Pipeline main started
{
        "message" => "192.168.1.18 - - [29/Aug/2018:15:00:38 +0800] \"GET / HTTP/1.1\" 200 3 \"-\" \"curl/7.29.0\"",
       "@version" => "1",
     "@timestamp" => "2018-08-29T08:18:40.800Z",
           "path" => "/tmp/alog",
           "host" => "logstash",
           "type" => "filelog",
       "clientip" => "192.168.1.18",
          "ident" => "-",
           "auth" => "-",
      "timestamp" => "29/Aug/2018:15:00:38 +0800",
           "verb" => "GET",
        "request" => "/",
    "httpversion" => "1.1",
       "response" => "200",
          "bytes" => "3",
       "referrer" => "\"-\"",
          "agent" => "\"curl/7.29.0\""
}


[root@logstash logstash]# cd  /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns
[root@logstash patterns]# ls
aws     bro   firewalls      haproxy  junos         mcollective           mongodb  postgresql  redis
bacula  exim  grok-patterns  java     linux-syslog  mcollective-patterns  nagios   rails       ruby
[root@logstash patterns]# vim grok-patterns 
USERNAME [a-zA-Z0-9._-]+
USER %{USERNAME}
EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+
EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME}
HTTPDUSER %{EMAILADDRESS}|%{USER}
INT (?:[+-]?(?:[0-9]+))
BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
NUMBER (?:%{BASE10NUM})
BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b

POSINT \b(?:[1-9][0-9]*)\b
NONNEGINT \b(?:[0-9]+)\b
WORD \b\w+\b
NOTSPACE \S+
SPACE \s*
DATA .*?
GREEDYDATA .*
QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}

# Networking
MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})

.....

# Log formats
SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
COMMONAPACHELOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
HTTPD20_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{LOGLEVEL:loglevel}\] (?:\[client %{IPORHOST:clientip}\] ){0,1}%{GREEDYDATA:errormsg}
HTTPD24_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{WORD:module}:%{LOGLEVEL:loglevel}\] \[pid %{POSINT:pid}:tid %{NUMBER:tid}\]( \(%{POSINT:proxy_errorcode}\)%{DATA:proxy_errormessage}:)?( \[client %{IPORHOST:client}:%{POSINT:clientport}\])? %{DATA:errorcode}: %{GREEDYDATA:message}
HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}

......

____________________________________________________________________________________

#配置客户端

[root@web01 ~]#  yum  -y  install filebeat-1.2.3-x86_64.rpm 

[root@web01 ~]#  vim  /etc/filebeat/filebeat.yml 
filebeat:
  prospectors:
    -
      paths:
        - /var/log/httpd/access_log
      input_type: log
      document_type: apachelog
  registry_file: /var/lib/filebeat/registry
output:
  logstash:
    hosts: ["192.168.1.17:5044"]     #IP为logstash的ip
shipper:
logging:
  files:
    rotateeverybytes: 10485760 # = 10MB

[root@web01 ~]# systemctl restart  filebeat.service

[root@web01 ~]# systemctl enable  filebeat.service

#配置logstash

[root@logstash logstash]# vim logstash.conf
input {
  stdin{ codec => "json" }
  file {
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb"
    path => ["/tmp/a.log"]
    type => "test_file_log"
  } 
  tcp {
    mode => "server"
    host => "0.0.0.0"
    port => 8888
    type => "tcplog"
  }
  udp {
    port => 8888
    type => "udplog"
  }
  syslog {
    port => 514
    type => "syslog"
  }
  beats{
    port => 5044
  }
}

filter{
  if [type] == "apachelog"{
  grok {
    match => ["message", "%{COMBINEDAPACHELOG}"]
  }}
}

output{
  stdout{ codec => "rubydebug" }
  if [type] == "apachelog"{
  elasticsearch {
    hosts => ["192.168.1.11:9200", "192.168.1.12:9200"]
    index => "weblog"
    flush_size => 2000
    idle_flush_time => 10
  }}
}

#浏览器访问192.168.1.18或者curl
[root@web01 ~]# curl  192.168.1.18
ok


[root@logstash logstash]# logstash -f  logstash.conf
Settings: Default pipeline workers: 2
Pipeline main started
{
        "message" => "192.168.1.254 - - [29/Aug/2018:17:36:26 +0800] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\"",
       "@version" => "1",
     "@timestamp" => "2018-08-29T09:36:33.448Z",
          "count" => 1,
           "beat" => {
        "hostname" => "web01",
            "name" => "web01"
    },
         "source" => "/var/log/httpd/access_log",
         "offset" => 1575,
           "type" => "apachelog",
         "fields" => nil,
     "input_type" => "log",
           "host" => "web01",
           "tags" => [
        [0] "beats_input_codec_plain_applied"
    ],
       "clientip" => "192.168.1.254",
          "ident" => "-",
           "auth" => "-",
      "timestamp" => "29/Aug/2018:17:36:26 +0800",
           "verb" => "GET",
        "request" => "/",
    "httpversion" => "1.1",
       "response" => "304",
       "referrer" => "\"-\"",
          "agent" => "\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\""
}




